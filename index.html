<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Research Paper Showcase</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
        }
        .container {
            width: 80%;
            max-width: 800px;
            margin: auto;
            overflow: hidden;
        }
        header {
            background: #333;
            color: #fff;
            padding: 20px 0;
            text-align: center;
        }
        header h1 {
            margin: 0;
            font-size: 2em;
        }
        .author-info {
            text-align: center;
            margin: 10px 0;
            font-size: 1.2em;
        }
        .main {
            padding: 20px;
            background: #fff;
            margin-top: 20px;
        }
        .section {
            margin-bottom: 20px;
        }
        h2 {
            color: #333;
            font-size: 1.5em;
            border-bottom: 2px solid #77aaff;
            padding-bottom: 10px;
        }
        p {
            font-size: 1em;
            line-height: 1.6;
            max-width: 100%;
            box-sizing: border-box;
            margin: 0 auto;
        }
        img {
            max-width: 100%;
            height: auto;
            display: block;
            margin: 0 auto;
        }
        .text-above {
            margin-bottom: 10px;
            font-style: italic;
            text-align: center;
        }
        .video-container {
            display: flex;
            align-items: center;
            justify-content: space-around;
            margin-top: 20px;
            position: relative;
        }
        .video-wrapper {
            position: relative;
            display: inline-block;
            text-align: center;
        }
        .video-wrapper video {
            max-width: 200px;
            height: auto;
        }
        .arrow {
            font-size: 2em;
            color: #000;
            z-index: 10;
        }
        .arrow-left {
            position: absolute;
            top: 50%;
            left: calc(50% - 150px);
            transform: translateY(-50%);
        }
        .arrow-right {
            position: absolute;
            top: 50%;
            right: calc(50% - 150px);
            transform: translateY(-50%);
        }
        .retrieving-video {
            position: absolute;
            top: -30px;
            left: -60px;
            font-size: 14px;
            color: red;
            white-space: nowrap;
        }
        .video-guide {
            position: absolute;
            top: -30px;
            left: 50%;
            transform: translateX(-50%);
            font-size: 14px;
            color: black;
            white-space: nowrap;
        }
        .text-below {
            text-align: center;
            margin-top: 10px;
        }
        footer {
            background: #333;
            color: #fff;
            text-align: center;
            padding: 10px 0;
            margin-top: 20px;
        }
        @media (max-width: 600px) {
            .container {
                width: 100%;
            }
            .video-wrapper video {
                max-width: 150px;
            }
            .arrow {
                font-size: 1.5em;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <header>
            <h1>Retrieving Video for Augmented Human Motion Generation</h1>
            <div class="author-info">
                <p>Beibei Jing, Youjia Zhang, Ruomiao Zhou, Zikai Song, Hefei Ling, Junqing Yu, Wei Yang<sup>*</sup></p>
                <p>Huazhong University of Science and Technology</p>
            </div>
        </header>
        <div class="section">
            <img src="FIGS/fig1.png" alt="Method">
            <p>
                RV2M first retrieves videos from Internet, more specifically YouTube, using input text prompt as query. For effective retrieval, we also optionally decompose the text into verb phrases and fetch anatomical videos and composite a reference video. We extract video features and use them to condition our the autoregressive motion generator for enhanced performances.
            </p>
        </div>
        <div class="main">
            <div class="section">
                <h2>Abstract</h2>
                <p>
                Text-driven human motion generation models, propelled by advancements in generative transformers and diffusion models, exhibit remarkable capabilities. However, they  still face considerable challenges due to a scarcity of motion capture data, leading to sub-optimal performance in responding to diverse, unconstrained prompts. In contrast, human activity videos are abundant on the Internet, surpass mocap data by orders of magnitude, yet remains under-utilized in motion generation. Drawing inspiration from the success of Retrieval Augmented Generation (RAG) in language modeling, this paper introduces a novel framework, RV2M, which integrates video retrieval into motion generation models to enhance their performance.Specifically, the framework incorporates a retrieval module designed to select reference videos from the Internet based on textual prompts. To address scenarios where no exact video match is found for a prompt, we propose a phrase-based retrieval and video composition scheme that maximizes the utilization of available video resources. Subsequently, we develop a video-conditioned autoregressive motion generator and formulate a training strategy that optimally leverages cross-modal data from videos to support the motion generation process.Extensive empirical evaluations reveal that our model outperforms existing state-of-the-art models across multiple metrics, and demonstrates robust adaptability to a wide range of expressions.
                </p>
            </div>
            <div class="section">
                <h2>Method</h2>
                <img src="FIGS/pipline.png" alt="Method">
                <p>
                    In the first phase, we train a text-to-motion generation model using residual VQ-VAE and masked modeling. In the second phase, we select long textual prompts in the Human3D dataset, and retrieve corresponding videos, we only keep panoptic videos with high relevancy for training. We extract features from videos for condition and use the ground-truth motion in dataset for supervision we retrieve corresponding video descriptions via text, extracting video and text features separately to input into the model trained in the first step for generating human motions.
                </p>
            </div>
            <h2>Results</h2>
            <div class="text-above">
                “a person dribbles a basketball through their legs then runs quickly”
            </div>
            <div class="video-container">
                <div class="video-wrapper">
                    <video controls>
                        <source src="video1.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="arrow arrow-right">→</div>
                <div class="video-wrapper">
                    <video controls>
                        <source src="video2.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
                <div class="arrow arrow-right">→</div>
                <div class="video-wrapper">
                    <video controls>
                        <source src="video3.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </div>
            </div>
            <div class="text-below">
                <a href="https://www.youtube.com/watch?v=t99XjrJrNPY">https://www.youtube.com/watch?v=t99XjrJrNPY</a>
            </div>
        </div>
        <footer>
            <p>&copy; 2024 Beibei Jing, Youjia Zhang, Ruomiao Zhou, Zikai Song, Hefei Ling, Junqing Yu, Wei Yang</p>
        </footer>
    </div>
</body>
</html>
